{
  "uiText": {
    "title": "How it works",
    "prev": "이전",
    "next": "다음"
  },
  "steps": [
    {
      "title": "1) 텍스트 입력 → 벡터화",
      "paragraphs": [
        "사용자가 문장을 입력하면, 모델은 문장을 작은 조각(토큰)으로 나누고 각 토큰을 숫자 벡터(임베딩)로 변환합니다.",
        "쉽게 말해, 글자를 컴퓨터가 이해할 수 있는 숫자로 바꾸는 과정입니다."
      ],
      "boldTexts": [
        "컴퓨터가 이해할 수 있는 숫자"
      ]
    },
    {
      "title": "2) 현재까지의 토큰(벡터)을 모델에 전달",
      "paragraphs": [
        "지금까지 만들어진 토큰 벡터들을 모델에 보내면, 모델은 이것을 \"문맥\"으로 사용해서 다음에 올 내용을 결정합니다.",
        "즉, 현재까지의 문맥을 모델에 넘겨주는 단계입니다."
      ],
      "boldTexts": [
        "현재까지의 문맥을 모델에 넘겨주는 단계"
      ]
    },
    {
      "title": "3) 다음 토큰 생성(한 단계씩)",
      "paragraphs": [
        "모델은 다음에 나올 토큰을 하나 고르고, 그 결과를 다시 문맥에 더합니다.",
        "이 과정을 반복하면 문장이 토큰 단위로 한 칸씩 길어집니다."
      ],
      "boldTexts": [
        "토큰 단위로 한 칸씩"
      ]
    },
    {
      "title": "4) 단어는 하위 단어 토큰으로 분할됩니다",
      "paragraphs": [
        "하나의 단어는 종종 토큰이라고 불리는 더 작은 하위 단어 조각으로 나뉩니다.",
        "예를 들어, \"GPT\"는 두 개의 별도 토큰으로 나타날 수 있습니다: \"G\"와 \"PT\"."
      ],
      "boldTexts": [
        "\"G\"와 \"PT\""
      ]
    },
    {
      "title": "5) 임베딩은 의미를 담고 있습니다",
      "paragraphs": [
        "각 토큰은 그 의미를 나타내는 고차원 임베딩 벡터로 변환됩니다.",
        "유사한 의미나 문맥을 가진 토큰들은 이 벡터 공간에서 서로 가까운 위치에 배치됩니다."
      ],
      "boldTexts": [
        "유사한 의미나 문맥",
        "서로 가까운 위치"
      ]
    },
    {
      "title": "6) 고차원 벡터를 3D로 시각화 (PCA)",
      "paragraphs": [
        "임베딩 벡터는 보통 매우 높은 차원(예: 2048차원)이라 그대로 보면 직관적으로 이해하기 어렵습니다.",
        "그래서 구조를 최대한 유지한 채 3차원으로 줄여 화면에 보여 줍니다. 이때 자주 쓰는 방법이 PCA(Wikipedia)입니다."
      ],
      "boldTexts": [
        "2048차원",
        "3차원",
        "PCA",
        "Wikipedia"
      ],
      "links": [
        {
          "text": "Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Principal_component_analysis"
        }
      ]
    }
  ]
}
