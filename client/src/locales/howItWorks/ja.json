{
  "uiText": {
    "title": "How it works",
    "prev": "前へ",
    "next": "次へ"
  },
  "steps": [
    {
      "title": "1) テキスト入力 → ベクトル化",
      "paragraphs": [
        "文を入力すると、モデルは文を小さなかたまり（トークン）に分け、各トークンを数値ベクトル（埋め込み）に変換します。",
        "かんたんに言うと、テキストをコンピュータが扱える数字に変えるイメージです。"
      ],
      "boldTexts": [
        "コンピュータが扱える数字"
      ]
    },
    {
      "title": "2) これまでのトークン（ベクトル）をモデルへ送る",
      "paragraphs": [
        "これまでにできたトークンベクトルをモデルに送ると、モデルはそれを「文脈」として使い、次に何が来るかを考えます。",
        "ここは、現在の文脈をモデルにわたすステップです。"
      ],
      "boldTexts": [
        "現在の文脈をモデルにわたすステップ"
      ]
    },
    {
      "title": "3) 次のトークンを生成（ステップごと）",
      "paragraphs": [
        "モデルは次のトークンを1つ選んで出力し、そのトークンをまた文脈にくわえます。",
        "このくり返しによって、文がトークン単位で少しずつ伸びていきます。"
      ],
      "boldTexts": [
        "トークン単位で少しずつ"
      ]
    },
    {
      "title": "4) 単語はサブワードトークンに分割されます",
      "paragraphs": [
        "1つの単語は、トークンと呼ばれるより小さなサブワードのかたまりに分解されることがよくあります。",
        "たとえば、「GPT」は2つの別々のトークンとして表示される場合があります：「G」と「PT」。"
      ],
      "boldTexts": [
        "「G」と「PT」"
      ]
    },
    {
      "title": "5) 埋め込みは意味を表現します",
      "paragraphs": [
        "各トークンは、その意味を表す高次元の埋め込みベクトルに変換されます。",
        "類似した意味や文脈を持つトークンは、このベクトル空間内で互いに近い位置に配置されます。"
      ],
      "boldTexts": [
        "類似した意味や文脈",
        "互いに近い位置"
      ]
    },
    {
      "title": "6) 高次元ベクトルを3Dで可視化（PCA）",
      "paragraphs": [
        "埋め込みベクトルはとても高次元（例：2048次元）なので、そのままだと直感的にイメージしづらいです。",
        "そこで構造をできるだけ保ったまま 3D に圧縮して表示します。よく使われる手法がPCA(Wikipedia)です。"
      ],
      "boldTexts": [
        "2048次元",
        "3D",
        "PCA",
        "Wikipedia"
      ],
      "links": [
        {
          "text": "Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Principal_component_analysis"
        }
      ]
    }
  ]
}
