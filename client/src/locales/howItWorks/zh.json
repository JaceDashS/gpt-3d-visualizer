{
  "uiText": {
    "title": "How it works",
    "prev": "上一页",
    "next": "下一页"
  },
  "steps": [
    {
      "title": "1) 输入文本 → 向量化",
      "paragraphs": [
        "当你输入一句话时，模型会把它切分成更小的片段（token），并把每个 token 转成数字向量（embedding）。",
        "简单说，就是把文字变成计算机可以处理的数字。"
      ],
      "boldTexts": [
        "计算机可以处理的数字"
      ]
    },
    {
      "title": "2) 把当前的 token（向量）发送给模型",
      "paragraphs": [
        "把目前已有的 token 向量发送给模型后，模型会把它们当成\"上下文\"，用来判断接下来应该出现什么。",
        "这个步骤就是把当前上下文交给模型。"
      ],
      "boldTexts": [
        "把当前上下文交给模型"
      ]
    },
    {
      "title": "3) 逐步生成下一个 token",
      "paragraphs": [
        "模型会选出下一个 token，把它输出出来，然后再把这个 token 加回上下文里。",
        "重复这样做，句子就会一个 token 一个 token 地生成出来。"
      ],
      "boldTexts": [
        "一个 token 一个 token 地"
      ]
    },
    {
      "title": "4) 单词被分割成子词 token",
      "paragraphs": [
        "一个单词通常会被分解成更小的子词片段，这些片段被称为 token。",
        "例如，\"GPT\" 可能会显示为两个独立的 token：\"G\" 和 \"PT\"。"
      ],
      "boldTexts": [
        "\"G\" 和 \"PT\""
      ]
    },
    {
      "title": "5) 嵌入向量捕捉语义",
      "paragraphs": [
        "每个 token 都会被转换成一个高维嵌入向量，用来表示它的含义。",
        "具有相似含义或上下文的 token 在这个向量空间中会被放置在彼此靠近的位置。"
      ],
      "boldTexts": [
        "相似含义或上下文",
        "彼此靠近的位置"
      ]
    },
    {
      "title": "6) 用 PCA 把高维向量可视化成 3D",
      "paragraphs": [
        "这些向量通常维度很高（例如2048 维），直接看并不直观。",
        "我们会在尽量保留结构的前提下，把它压缩到3D来展示。常用的方法之一是PCA(Wikipedia)。"
      ],
      "boldTexts": [
        "2048 维",
        "3D",
        "PCA",
        "Wikipedia"
      ],
      "links": [
        {
          "text": "Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Principal_component_analysis"
        }
      ]
    }
  ]
}
