{
  "uiText": {
    "title": "How it works",
    "prev": "Prev",
    "next": "Next"
  },
  "steps": [
    {
      "title": "1) Enter text → vectorize",
      "paragraphs": [
        "When you enter a sentence, the model splits it into small pieces (tokens) and converts each token into a numeric vector (embedding).",
        "Simply put, it's turning text into numbers."
      ],
      "boldTexts": [
        "turning text into numbers"
      ]
    },
    {
      "title": "2) Send tokens (vectors) to the model",
      "paragraphs": [
        "The model receives the vectors built so far and uses them as \"context\" to decide what should come next.",
        "This is the step where we pass the current context to the model."
      ],
      "boldTexts": [
        "pass the current context to the model"
      ]
    },
    {
      "title": "3) Generate the next token",
      "paragraphs": [
        "The model picks the next token, outputs it, and then adds it back into the context.",
        "Repeating this produces the sentence one token at a time."
      ]
    },
    {
      "title": "4) Words are split into subword tokens",
      "paragraphs": [
        "A single word is often broken down into smaller subword pieces called tokens.",
        "For example, \"GPT\" might appear as two separate tokens: \"G\" and \"PT\"."
      ],
      "boldTexts": [
        "\"G\" and \"PT\""
      ]
    },
    {
      "title": "5) Embeddings capture semantic meaning",
      "paragraphs": [
        "Each token is converted into a high-dimensional embedding vector that represents its meaning.",
        "Tokens with similar meanings or contexts are positioned close to each other in this vector space."
      ],
      "boldTexts": [
        "similar meanings or contexts",
        "close to each other"
      ]
    },
    {
      "title": "6) Visualize high‑D vectors in 3D (PCA)",
      "paragraphs": [
        "Embedding vectors are usually very high-dimensional (e.g. 2048D), so they're not very intuitive on their own.",
        "We reduce them to 3D for visualization while keeping as much structure as possible. A common method is PCA(Wikipedia)."
      ],
      "boldTexts": [
        "2048D",
        "3D",
        "PCA",
        "Wikipedia"
      ],
      "links": [
        {
          "text": "Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Principal_component_analysis"
        }
      ]
    }
  ]
}
